#include "kernel_col.h"



#include <stdio.h>
#include <stdlib.h>

#include <arm_neon.h>


// gemm_NEON_10x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 10] @DRAM
// )
void gemm_NEON_10x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
free(C);
}

// gemm_NEON_10x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 10] @DRAM
// )
void gemm_NEON_10x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
free(C);
}

// gemm_NEON_10x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 10] @DRAM
// )
void gemm_NEON_10x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
free(C);
}

// gemm_NEON_10x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 10] @DRAM
// )
void gemm_NEON_10x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
free(C);
}

// gemm_NEON_10x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 10] @DRAM
// )
void gemm_NEON_10x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[2 * Ci.strides[0] + 9] = C[41];
free(C);
}

// gemm_NEON_10x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 10] @DRAM
// )
void gemm_NEON_10x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[2 * Ci.strides[0] + 9] += C[41];
free(C);
}

// gemm_NEON_10x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 10] @DRAM
// )
void gemm_NEON_10x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[2 * Ci.strides[0] + 9] = C[41];
Ci.data[3 * Ci.strides[0]] = C[48];
Ci.data[3 * Ci.strides[0] + 1] = C[49];
Ci.data[3 * Ci.strides[0] + 2] = C[50];
Ci.data[3 * Ci.strides[0] + 3] = C[51];
Ci.data[3 * Ci.strides[0] + 4] = C[52];
Ci.data[3 * Ci.strides[0] + 5] = C[53];
Ci.data[3 * Ci.strides[0] + 6] = C[54];
Ci.data[3 * Ci.strides[0] + 7] = C[55];
Ci.data[3 * Ci.strides[0] + 8] = C[56];
Ci.data[3 * Ci.strides[0] + 9] = C[57];
free(C);
}

// gemm_NEON_10x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 10] @DRAM
// )
void gemm_NEON_10x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[2 * Ci.strides[0] + 9] += C[41];
Ci.data[3 * Ci.strides[0]] += C[48];
Ci.data[3 * Ci.strides[0] + 1] += C[49];
Ci.data[3 * Ci.strides[0] + 2] += C[50];
Ci.data[3 * Ci.strides[0] + 3] += C[51];
Ci.data[3 * Ci.strides[0] + 4] += C[52];
Ci.data[3 * Ci.strides[0] + 5] += C[53];
Ci.data[3 * Ci.strides[0] + 6] += C[54];
Ci.data[3 * Ci.strides[0] + 7] += C[55];
Ci.data[3 * Ci.strides[0] + 8] += C[56];
Ci.data[3 * Ci.strides[0] + 9] += C[57];
free(C);
}

// gemm_NEON_11x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 11] @DRAM
// )
void gemm_NEON_11x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
free(C);
}

// gemm_NEON_11x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 11] @DRAM
// )
void gemm_NEON_11x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
free(C);
}

// gemm_NEON_11x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 11] @DRAM
// )
void gemm_NEON_11x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
free(C);
}

// gemm_NEON_11x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 11] @DRAM
// )
void gemm_NEON_11x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
free(C);
}

// gemm_NEON_11x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 11] @DRAM
// )
void gemm_NEON_11x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[2 * Ci.strides[0] + 9] = C[41];
Ci.data[2 * Ci.strides[0] + 10] = C[42];
free(C);
}

// gemm_NEON_11x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 11] @DRAM
// )
void gemm_NEON_11x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[2 * Ci.strides[0] + 9] += C[41];
Ci.data[2 * Ci.strides[0] + 10] += C[42];
free(C);
}

// gemm_NEON_11x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 11] @DRAM
// )
void gemm_NEON_11x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[2 * Ci.strides[0] + 9] = C[41];
Ci.data[2 * Ci.strides[0] + 10] = C[42];
Ci.data[3 * Ci.strides[0]] = C[48];
Ci.data[3 * Ci.strides[0] + 1] = C[49];
Ci.data[3 * Ci.strides[0] + 2] = C[50];
Ci.data[3 * Ci.strides[0] + 3] = C[51];
Ci.data[3 * Ci.strides[0] + 4] = C[52];
Ci.data[3 * Ci.strides[0] + 5] = C[53];
Ci.data[3 * Ci.strides[0] + 6] = C[54];
Ci.data[3 * Ci.strides[0] + 7] = C[55];
Ci.data[3 * Ci.strides[0] + 8] = C[56];
Ci.data[3 * Ci.strides[0] + 9] = C[57];
Ci.data[3 * Ci.strides[0] + 10] = C[58];
free(C);
}

// gemm_NEON_11x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 11] @DRAM
// )
void gemm_NEON_11x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[2 * Ci.strides[0] + 9] += C[41];
Ci.data[2 * Ci.strides[0] + 10] += C[42];
Ci.data[3 * Ci.strides[0]] += C[48];
Ci.data[3 * Ci.strides[0] + 1] += C[49];
Ci.data[3 * Ci.strides[0] + 2] += C[50];
Ci.data[3 * Ci.strides[0] + 3] += C[51];
Ci.data[3 * Ci.strides[0] + 4] += C[52];
Ci.data[3 * Ci.strides[0] + 5] += C[53];
Ci.data[3 * Ci.strides[0] + 6] += C[54];
Ci.data[3 * Ci.strides[0] + 7] += C[55];
Ci.data[3 * Ci.strides[0] + 8] += C[56];
Ci.data[3 * Ci.strides[0] + 9] += C[57];
Ci.data[3 * Ci.strides[0] + 10] += C[58];
free(C);
}

// gemm_NEON_12x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 12] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 12] @DRAM
// )
void gemm_NEON_12x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 12
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 12 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_1_0);
vst1q_s32(&C[12 + 4], C_reg_1_1);
vst1q_s32(&C[12 + 8], C_reg_1_2);
vst1q_s32(&C[(2) * (12)], C_reg_2_0);
vst1q_s32(&C[(2) * (12) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (12) + 8], C_reg_2_2);
vst1q_s32(&C[(3) * (12)], C_reg_3_0);
vst1q_s32(&C[(3) * (12) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (12) + 8], C_reg_3_2);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
free(C);
}

// gemm_NEON_12x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 12] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 12] @DRAM
// )
void gemm_NEON_12x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 12
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 12 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_1_0);
vst1q_s32(&C[12 + 4], C_reg_1_1);
vst1q_s32(&C[12 + 8], C_reg_1_2);
vst1q_s32(&C[(2) * (12)], C_reg_2_0);
vst1q_s32(&C[(2) * (12) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (12) + 8], C_reg_2_2);
vst1q_s32(&C[(3) * (12)], C_reg_3_0);
vst1q_s32(&C[(3) * (12) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (12) + 8], C_reg_3_2);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
free(C);
}

// gemm_NEON_12x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 12] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 12] @DRAM
// )
void gemm_NEON_12x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 12
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 12 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_1_0);
vst1q_s32(&C[12 + 4], C_reg_1_1);
vst1q_s32(&C[12 + 8], C_reg_1_2);
vst1q_s32(&C[(2) * (12)], C_reg_2_0);
vst1q_s32(&C[(2) * (12) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (12) + 8], C_reg_2_2);
vst1q_s32(&C[(3) * (12)], C_reg_3_0);
vst1q_s32(&C[(3) * (12) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (12) + 8], C_reg_3_2);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[Ci.strides[0]] = C[12];
Ci.data[Ci.strides[0] + 1] = C[13];
Ci.data[Ci.strides[0] + 2] = C[14];
Ci.data[Ci.strides[0] + 3] = C[15];
Ci.data[Ci.strides[0] + 4] = C[16];
Ci.data[Ci.strides[0] + 5] = C[17];
Ci.data[Ci.strides[0] + 6] = C[18];
Ci.data[Ci.strides[0] + 7] = C[19];
Ci.data[Ci.strides[0] + 8] = C[20];
Ci.data[Ci.strides[0] + 9] = C[21];
Ci.data[Ci.strides[0] + 10] = C[22];
Ci.data[Ci.strides[0] + 11] = C[23];
free(C);
}

// gemm_NEON_12x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 12] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 12] @DRAM
// )
void gemm_NEON_12x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 12
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 12 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_1_0);
vst1q_s32(&C[12 + 4], C_reg_1_1);
vst1q_s32(&C[12 + 8], C_reg_1_2);
vst1q_s32(&C[(2) * (12)], C_reg_2_0);
vst1q_s32(&C[(2) * (12) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (12) + 8], C_reg_2_2);
vst1q_s32(&C[(3) * (12)], C_reg_3_0);
vst1q_s32(&C[(3) * (12) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (12) + 8], C_reg_3_2);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[Ci.strides[0]] += C[12];
Ci.data[Ci.strides[0] + 1] += C[13];
Ci.data[Ci.strides[0] + 2] += C[14];
Ci.data[Ci.strides[0] + 3] += C[15];
Ci.data[Ci.strides[0] + 4] += C[16];
Ci.data[Ci.strides[0] + 5] += C[17];
Ci.data[Ci.strides[0] + 6] += C[18];
Ci.data[Ci.strides[0] + 7] += C[19];
Ci.data[Ci.strides[0] + 8] += C[20];
Ci.data[Ci.strides[0] + 9] += C[21];
Ci.data[Ci.strides[0] + 10] += C[22];
Ci.data[Ci.strides[0] + 11] += C[23];
free(C);
}

// gemm_NEON_12x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 12] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 12] @DRAM
// )
void gemm_NEON_12x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 12
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 12 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_1_0);
vst1q_s32(&C[12 + 4], C_reg_1_1);
vst1q_s32(&C[12 + 8], C_reg_1_2);
vst1q_s32(&C[(2) * (12)], C_reg_2_0);
vst1q_s32(&C[(2) * (12) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (12) + 8], C_reg_2_2);
vst1q_s32(&C[(3) * (12)], C_reg_3_0);
vst1q_s32(&C[(3) * (12) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (12) + 8], C_reg_3_2);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[Ci.strides[0]] = C[12];
Ci.data[Ci.strides[0] + 1] = C[13];
Ci.data[Ci.strides[0] + 2] = C[14];
Ci.data[Ci.strides[0] + 3] = C[15];
Ci.data[Ci.strides[0] + 4] = C[16];
Ci.data[Ci.strides[0] + 5] = C[17];
Ci.data[Ci.strides[0] + 6] = C[18];
Ci.data[Ci.strides[0] + 7] = C[19];
Ci.data[Ci.strides[0] + 8] = C[20];
Ci.data[Ci.strides[0] + 9] = C[21];
Ci.data[Ci.strides[0] + 10] = C[22];
Ci.data[Ci.strides[0] + 11] = C[23];
Ci.data[2 * Ci.strides[0]] = C[24];
Ci.data[2 * Ci.strides[0] + 1] = C[25];
Ci.data[2 * Ci.strides[0] + 2] = C[26];
Ci.data[2 * Ci.strides[0] + 3] = C[27];
Ci.data[2 * Ci.strides[0] + 4] = C[28];
Ci.data[2 * Ci.strides[0] + 5] = C[29];
Ci.data[2 * Ci.strides[0] + 6] = C[30];
Ci.data[2 * Ci.strides[0] + 7] = C[31];
Ci.data[2 * Ci.strides[0] + 8] = C[32];
Ci.data[2 * Ci.strides[0] + 9] = C[33];
Ci.data[2 * Ci.strides[0] + 10] = C[34];
Ci.data[2 * Ci.strides[0] + 11] = C[35];
free(C);
}

// gemm_NEON_12x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 12] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 12] @DRAM
// )
void gemm_NEON_12x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 12
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 12 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (12) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_1_0);
vst1q_s32(&C[12 + 4], C_reg_1_1);
vst1q_s32(&C[12 + 8], C_reg_1_2);
vst1q_s32(&C[(2) * (12)], C_reg_2_0);
vst1q_s32(&C[(2) * (12) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (12) + 8], C_reg_2_2);
vst1q_s32(&C[(3) * (12)], C_reg_3_0);
vst1q_s32(&C[(3) * (12) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (12) + 8], C_reg_3_2);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[Ci.strides[0]] += C[12];
Ci.data[Ci.strides[0] + 1] += C[13];
Ci.data[Ci.strides[0] + 2] += C[14];
Ci.data[Ci.strides[0] + 3] += C[15];
Ci.data[Ci.strides[0] + 4] += C[16];
Ci.data[Ci.strides[0] + 5] += C[17];
Ci.data[Ci.strides[0] + 6] += C[18];
Ci.data[Ci.strides[0] + 7] += C[19];
Ci.data[Ci.strides[0] + 8] += C[20];
Ci.data[Ci.strides[0] + 9] += C[21];
Ci.data[Ci.strides[0] + 10] += C[22];
Ci.data[Ci.strides[0] + 11] += C[23];
Ci.data[2 * Ci.strides[0]] += C[24];
Ci.data[2 * Ci.strides[0] + 1] += C[25];
Ci.data[2 * Ci.strides[0] + 2] += C[26];
Ci.data[2 * Ci.strides[0] + 3] += C[27];
Ci.data[2 * Ci.strides[0] + 4] += C[28];
Ci.data[2 * Ci.strides[0] + 5] += C[29];
Ci.data[2 * Ci.strides[0] + 6] += C[30];
Ci.data[2 * Ci.strides[0] + 7] += C[31];
Ci.data[2 * Ci.strides[0] + 8] += C[32];
Ci.data[2 * Ci.strides[0] + 9] += C[33];
Ci.data[2 * Ci.strides[0] + 10] += C[34];
Ci.data[2 * Ci.strides[0] + 11] += C[35];
free(C);
}

// gemm_NEON_12x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 12] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     beta : i32[1] @DRAM,
//     C : [i32][4, 12] @DRAM
// )
void gemm_NEON_12x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* beta, struct exo_win_2i32 C ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
vst1q_s32(&C.data[0], C_reg_0_0);
vst1q_s32(&C.data[4], C_reg_0_1);
vst1q_s32(&C.data[8], C_reg_0_2);
vst1q_s32(&C.data[C.strides[0]], C_reg_1_0);
vst1q_s32(&C.data[C.strides[0] + 4], C_reg_1_1);
vst1q_s32(&C.data[C.strides[0] + 8], C_reg_1_2);
vst1q_s32(&C.data[(2) * (C.strides[0])], C_reg_2_0);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 4], C_reg_2_1);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_2);
vst1q_s32(&C.data[(3) * (C.strides[0])], C_reg_3_0);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 4], C_reg_3_1);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_2);
}

// gemm_NEON_12x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 12] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     beta : i32[1] @DRAM,
//     C : [i32][4, 12] @DRAM
// )
void gemm_NEON_12x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* beta, struct exo_win_2i32 C ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
C_reg_0_0 = vld1q_s32(&C.data[0]);
C_reg_0_1 = vld1q_s32(&C.data[4]);
C_reg_0_2 = vld1q_s32(&C.data[8]);
C_reg_1_0 = vld1q_s32(&C.data[C.strides[0]]);
C_reg_1_1 = vld1q_s32(&C.data[C.strides[0] + 4]);
C_reg_1_2 = vld1q_s32(&C.data[C.strides[0] + 8]);
C_reg_2_0 = vld1q_s32(&C.data[(2) * (C.strides[0])]);
C_reg_2_1 = vld1q_s32(&C.data[(2) * (C.strides[0]) + 4]);
C_reg_2_2 = vld1q_s32(&C.data[(2) * (C.strides[0]) + 8]);
C_reg_3_0 = vld1q_s32(&C.data[(3) * (C.strides[0])]);
C_reg_3_1 = vld1q_s32(&C.data[(3) * (C.strides[0]) + 4]);
C_reg_3_2 = vld1q_s32(&C.data[(3) * (C.strides[0]) + 8]);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
}
vst1q_s32(&C.data[0], C_reg_0_0);
vst1q_s32(&C.data[4], C_reg_0_1);
vst1q_s32(&C.data[8], C_reg_0_2);
vst1q_s32(&C.data[C.strides[0]], C_reg_1_0);
vst1q_s32(&C.data[C.strides[0] + 4], C_reg_1_1);
vst1q_s32(&C.data[C.strides[0] + 8], C_reg_1_2);
vst1q_s32(&C.data[(2) * (C.strides[0])], C_reg_2_0);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 4], C_reg_2_1);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_2);
vst1q_s32(&C.data[(3) * (C.strides[0])], C_reg_3_0);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 4], C_reg_3_1);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_2);
}

// gemm_NEON_13x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 13] @DRAM
// )
void gemm_NEON_13x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
free(C);
}

// gemm_NEON_13x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 13] @DRAM
// )
void gemm_NEON_13x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
free(C);
}

// gemm_NEON_13x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 13] @DRAM
// )
void gemm_NEON_13x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[Ci.strides[0] + 11] = C[27];
Ci.data[Ci.strides[0] + 12] = C[28];
free(C);
}

// gemm_NEON_13x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 13] @DRAM
// )
void gemm_NEON_13x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[Ci.strides[0] + 11] += C[27];
Ci.data[Ci.strides[0] + 12] += C[28];
free(C);
}

// gemm_NEON_13x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 13] @DRAM
// )
void gemm_NEON_13x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[Ci.strides[0] + 11] = C[27];
Ci.data[Ci.strides[0] + 12] = C[28];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[2 * Ci.strides[0] + 9] = C[41];
Ci.data[2 * Ci.strides[0] + 10] = C[42];
Ci.data[2 * Ci.strides[0] + 11] = C[43];
Ci.data[2 * Ci.strides[0] + 12] = C[44];
free(C);
}

// gemm_NEON_13x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 13] @DRAM
// )
void gemm_NEON_13x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[Ci.strides[0] + 11] += C[27];
Ci.data[Ci.strides[0] + 12] += C[28];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[2 * Ci.strides[0] + 9] += C[41];
Ci.data[2 * Ci.strides[0] + 10] += C[42];
Ci.data[2 * Ci.strides[0] + 11] += C[43];
Ci.data[2 * Ci.strides[0] + 12] += C[44];
free(C);
}

// gemm_NEON_13x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 13] @DRAM
// )
void gemm_NEON_13x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[Ci.strides[0] + 11] = C[27];
Ci.data[Ci.strides[0] + 12] = C[28];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[2 * Ci.strides[0] + 9] = C[41];
Ci.data[2 * Ci.strides[0] + 10] = C[42];
Ci.data[2 * Ci.strides[0] + 11] = C[43];
Ci.data[2 * Ci.strides[0] + 12] = C[44];
Ci.data[3 * Ci.strides[0]] = C[48];
Ci.data[3 * Ci.strides[0] + 1] = C[49];
Ci.data[3 * Ci.strides[0] + 2] = C[50];
Ci.data[3 * Ci.strides[0] + 3] = C[51];
Ci.data[3 * Ci.strides[0] + 4] = C[52];
Ci.data[3 * Ci.strides[0] + 5] = C[53];
Ci.data[3 * Ci.strides[0] + 6] = C[54];
Ci.data[3 * Ci.strides[0] + 7] = C[55];
Ci.data[3 * Ci.strides[0] + 8] = C[56];
Ci.data[3 * Ci.strides[0] + 9] = C[57];
Ci.data[3 * Ci.strides[0] + 10] = C[58];
Ci.data[3 * Ci.strides[0] + 11] = C[59];
Ci.data[3 * Ci.strides[0] + 12] = C[60];
free(C);
}

// gemm_NEON_13x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 13] @DRAM
// )
void gemm_NEON_13x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[Ci.strides[0] + 11] += C[27];
Ci.data[Ci.strides[0] + 12] += C[28];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[2 * Ci.strides[0] + 9] += C[41];
Ci.data[2 * Ci.strides[0] + 10] += C[42];
Ci.data[2 * Ci.strides[0] + 11] += C[43];
Ci.data[2 * Ci.strides[0] + 12] += C[44];
Ci.data[3 * Ci.strides[0]] += C[48];
Ci.data[3 * Ci.strides[0] + 1] += C[49];
Ci.data[3 * Ci.strides[0] + 2] += C[50];
Ci.data[3 * Ci.strides[0] + 3] += C[51];
Ci.data[3 * Ci.strides[0] + 4] += C[52];
Ci.data[3 * Ci.strides[0] + 5] += C[53];
Ci.data[3 * Ci.strides[0] + 6] += C[54];
Ci.data[3 * Ci.strides[0] + 7] += C[55];
Ci.data[3 * Ci.strides[0] + 8] += C[56];
Ci.data[3 * Ci.strides[0] + 9] += C[57];
Ci.data[3 * Ci.strides[0] + 10] += C[58];
Ci.data[3 * Ci.strides[0] + 11] += C[59];
Ci.data[3 * Ci.strides[0] + 12] += C[60];
free(C);
}

// gemm_NEON_14x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 14] @DRAM
// )
void gemm_NEON_14x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[13] = C[13];
free(C);
}

// gemm_NEON_14x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 14] @DRAM
// )
void gemm_NEON_14x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[13] += C[13];
free(C);
}

// gemm_NEON_14x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 14] @DRAM
// )
void gemm_NEON_14x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[13] = C[13];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[Ci.strides[0] + 11] = C[27];
Ci.data[Ci.strides[0] + 12] = C[28];
Ci.data[Ci.strides[0] + 13] = C[29];
free(C);
}

// gemm_NEON_14x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 14] @DRAM
// )
void gemm_NEON_14x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[13] += C[13];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[Ci.strides[0] + 11] += C[27];
Ci.data[Ci.strides[0] + 12] += C[28];
Ci.data[Ci.strides[0] + 13] += C[29];
free(C);
}

// gemm_NEON_14x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 14] @DRAM
// )
void gemm_NEON_14x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[13] = C[13];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[Ci.strides[0] + 11] = C[27];
Ci.data[Ci.strides[0] + 12] = C[28];
Ci.data[Ci.strides[0] + 13] = C[29];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[2 * Ci.strides[0] + 9] = C[41];
Ci.data[2 * Ci.strides[0] + 10] = C[42];
Ci.data[2 * Ci.strides[0] + 11] = C[43];
Ci.data[2 * Ci.strides[0] + 12] = C[44];
Ci.data[2 * Ci.strides[0] + 13] = C[45];
free(C);
}

// gemm_NEON_14x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 14] @DRAM
// )
void gemm_NEON_14x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[13] += C[13];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[Ci.strides[0] + 11] += C[27];
Ci.data[Ci.strides[0] + 12] += C[28];
Ci.data[Ci.strides[0] + 13] += C[29];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[2 * Ci.strides[0] + 9] += C[41];
Ci.data[2 * Ci.strides[0] + 10] += C[42];
Ci.data[2 * Ci.strides[0] + 11] += C[43];
Ci.data[2 * Ci.strides[0] + 12] += C[44];
Ci.data[2 * Ci.strides[0] + 13] += C[45];
free(C);
}

// gemm_NEON_14x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 14] @DRAM
// )
void gemm_NEON_14x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[13] = C[13];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[Ci.strides[0] + 11] = C[27];
Ci.data[Ci.strides[0] + 12] = C[28];
Ci.data[Ci.strides[0] + 13] = C[29];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[2 * Ci.strides[0] + 9] = C[41];
Ci.data[2 * Ci.strides[0] + 10] = C[42];
Ci.data[2 * Ci.strides[0] + 11] = C[43];
Ci.data[2 * Ci.strides[0] + 12] = C[44];
Ci.data[2 * Ci.strides[0] + 13] = C[45];
Ci.data[3 * Ci.strides[0]] = C[48];
Ci.data[3 * Ci.strides[0] + 1] = C[49];
Ci.data[3 * Ci.strides[0] + 2] = C[50];
Ci.data[3 * Ci.strides[0] + 3] = C[51];
Ci.data[3 * Ci.strides[0] + 4] = C[52];
Ci.data[3 * Ci.strides[0] + 5] = C[53];
Ci.data[3 * Ci.strides[0] + 6] = C[54];
Ci.data[3 * Ci.strides[0] + 7] = C[55];
Ci.data[3 * Ci.strides[0] + 8] = C[56];
Ci.data[3 * Ci.strides[0] + 9] = C[57];
Ci.data[3 * Ci.strides[0] + 10] = C[58];
Ci.data[3 * Ci.strides[0] + 11] = C[59];
Ci.data[3 * Ci.strides[0] + 12] = C[60];
Ci.data[3 * Ci.strides[0] + 13] = C[61];
free(C);
}

// gemm_NEON_14x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 14] @DRAM
// )
void gemm_NEON_14x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[13] += C[13];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[Ci.strides[0] + 11] += C[27];
Ci.data[Ci.strides[0] + 12] += C[28];
Ci.data[Ci.strides[0] + 13] += C[29];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[2 * Ci.strides[0] + 9] += C[41];
Ci.data[2 * Ci.strides[0] + 10] += C[42];
Ci.data[2 * Ci.strides[0] + 11] += C[43];
Ci.data[2 * Ci.strides[0] + 12] += C[44];
Ci.data[2 * Ci.strides[0] + 13] += C[45];
Ci.data[3 * Ci.strides[0]] += C[48];
Ci.data[3 * Ci.strides[0] + 1] += C[49];
Ci.data[3 * Ci.strides[0] + 2] += C[50];
Ci.data[3 * Ci.strides[0] + 3] += C[51];
Ci.data[3 * Ci.strides[0] + 4] += C[52];
Ci.data[3 * Ci.strides[0] + 5] += C[53];
Ci.data[3 * Ci.strides[0] + 6] += C[54];
Ci.data[3 * Ci.strides[0] + 7] += C[55];
Ci.data[3 * Ci.strides[0] + 8] += C[56];
Ci.data[3 * Ci.strides[0] + 9] += C[57];
Ci.data[3 * Ci.strides[0] + 10] += C[58];
Ci.data[3 * Ci.strides[0] + 11] += C[59];
Ci.data[3 * Ci.strides[0] + 12] += C[60];
Ci.data[3 * Ci.strides[0] + 13] += C[61];
free(C);
}

// gemm_NEON_15x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 15] @DRAM
// )
void gemm_NEON_15x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[13] = C[13];
Ci.data[14] = C[14];
free(C);
}

// gemm_NEON_15x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 15] @DRAM
// )
void gemm_NEON_15x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[13] += C[13];
Ci.data[14] += C[14];
free(C);
}

// gemm_NEON_15x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 15] @DRAM
// )
void gemm_NEON_15x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[13] = C[13];
Ci.data[14] = C[14];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[Ci.strides[0] + 11] = C[27];
Ci.data[Ci.strides[0] + 12] = C[28];
Ci.data[Ci.strides[0] + 13] = C[29];
Ci.data[Ci.strides[0] + 14] = C[30];
free(C);
}

// gemm_NEON_15x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 15] @DRAM
// )
void gemm_NEON_15x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[13] += C[13];
Ci.data[14] += C[14];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[Ci.strides[0] + 11] += C[27];
Ci.data[Ci.strides[0] + 12] += C[28];
Ci.data[Ci.strides[0] + 13] += C[29];
Ci.data[Ci.strides[0] + 14] += C[30];
free(C);
}

// gemm_NEON_15x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 15] @DRAM
// )
void gemm_NEON_15x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[13] = C[13];
Ci.data[14] = C[14];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[Ci.strides[0] + 11] = C[27];
Ci.data[Ci.strides[0] + 12] = C[28];
Ci.data[Ci.strides[0] + 13] = C[29];
Ci.data[Ci.strides[0] + 14] = C[30];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[2 * Ci.strides[0] + 9] = C[41];
Ci.data[2 * Ci.strides[0] + 10] = C[42];
Ci.data[2 * Ci.strides[0] + 11] = C[43];
Ci.data[2 * Ci.strides[0] + 12] = C[44];
Ci.data[2 * Ci.strides[0] + 13] = C[45];
Ci.data[2 * Ci.strides[0] + 14] = C[46];
free(C);
}

// gemm_NEON_15x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 15] @DRAM
// )
void gemm_NEON_15x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[13] += C[13];
Ci.data[14] += C[14];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[Ci.strides[0] + 11] += C[27];
Ci.data[Ci.strides[0] + 12] += C[28];
Ci.data[Ci.strides[0] + 13] += C[29];
Ci.data[Ci.strides[0] + 14] += C[30];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[2 * Ci.strides[0] + 9] += C[41];
Ci.data[2 * Ci.strides[0] + 10] += C[42];
Ci.data[2 * Ci.strides[0] + 11] += C[43];
Ci.data[2 * Ci.strides[0] + 12] += C[44];
Ci.data[2 * Ci.strides[0] + 13] += C[45];
Ci.data[2 * Ci.strides[0] + 14] += C[46];
free(C);
}

// gemm_NEON_15x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 15] @DRAM
// )
void gemm_NEON_15x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[13] = C[13];
Ci.data[14] = C[14];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[Ci.strides[0] + 11] = C[27];
Ci.data[Ci.strides[0] + 12] = C[28];
Ci.data[Ci.strides[0] + 13] = C[29];
Ci.data[Ci.strides[0] + 14] = C[30];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[2 * Ci.strides[0] + 9] = C[41];
Ci.data[2 * Ci.strides[0] + 10] = C[42];
Ci.data[2 * Ci.strides[0] + 11] = C[43];
Ci.data[2 * Ci.strides[0] + 12] = C[44];
Ci.data[2 * Ci.strides[0] + 13] = C[45];
Ci.data[2 * Ci.strides[0] + 14] = C[46];
Ci.data[3 * Ci.strides[0]] = C[48];
Ci.data[3 * Ci.strides[0] + 1] = C[49];
Ci.data[3 * Ci.strides[0] + 2] = C[50];
Ci.data[3 * Ci.strides[0] + 3] = C[51];
Ci.data[3 * Ci.strides[0] + 4] = C[52];
Ci.data[3 * Ci.strides[0] + 5] = C[53];
Ci.data[3 * Ci.strides[0] + 6] = C[54];
Ci.data[3 * Ci.strides[0] + 7] = C[55];
Ci.data[3 * Ci.strides[0] + 8] = C[56];
Ci.data[3 * Ci.strides[0] + 9] = C[57];
Ci.data[3 * Ci.strides[0] + 10] = C[58];
Ci.data[3 * Ci.strides[0] + 11] = C[59];
Ci.data[3 * Ci.strides[0] + 12] = C[60];
Ci.data[3 * Ci.strides[0] + 13] = C[61];
Ci.data[3 * Ci.strides[0] + 14] = C[62];
free(C);
}

// gemm_NEON_15x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 15] @DRAM
// )
void gemm_NEON_15x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[13] += C[13];
Ci.data[14] += C[14];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[Ci.strides[0] + 11] += C[27];
Ci.data[Ci.strides[0] + 12] += C[28];
Ci.data[Ci.strides[0] + 13] += C[29];
Ci.data[Ci.strides[0] + 14] += C[30];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[2 * Ci.strides[0] + 9] += C[41];
Ci.data[2 * Ci.strides[0] + 10] += C[42];
Ci.data[2 * Ci.strides[0] + 11] += C[43];
Ci.data[2 * Ci.strides[0] + 12] += C[44];
Ci.data[2 * Ci.strides[0] + 13] += C[45];
Ci.data[2 * Ci.strides[0] + 14] += C[46];
Ci.data[3 * Ci.strides[0]] += C[48];
Ci.data[3 * Ci.strides[0] + 1] += C[49];
Ci.data[3 * Ci.strides[0] + 2] += C[50];
Ci.data[3 * Ci.strides[0] + 3] += C[51];
Ci.data[3 * Ci.strides[0] + 4] += C[52];
Ci.data[3 * Ci.strides[0] + 5] += C[53];
Ci.data[3 * Ci.strides[0] + 6] += C[54];
Ci.data[3 * Ci.strides[0] + 7] += C[55];
Ci.data[3 * Ci.strides[0] + 8] += C[56];
Ci.data[3 * Ci.strides[0] + 9] += C[57];
Ci.data[3 * Ci.strides[0] + 10] += C[58];
Ci.data[3 * Ci.strides[0] + 11] += C[59];
Ci.data[3 * Ci.strides[0] + 12] += C[60];
Ci.data[3 * Ci.strides[0] + 13] += C[61];
Ci.data[3 * Ci.strides[0] + 14] += C[62];
free(C);
}

// gemm_NEON_16x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 16] @DRAM
// )
void gemm_NEON_16x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[13] = C[13];
Ci.data[14] = C[14];
Ci.data[15] = C[15];
free(C);
}

// gemm_NEON_16x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 16] @DRAM
// )
void gemm_NEON_16x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[13] += C[13];
Ci.data[14] += C[14];
Ci.data[15] += C[15];
free(C);
}

// gemm_NEON_16x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 16] @DRAM
// )
void gemm_NEON_16x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[13] = C[13];
Ci.data[14] = C[14];
Ci.data[15] = C[15];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[Ci.strides[0] + 11] = C[27];
Ci.data[Ci.strides[0] + 12] = C[28];
Ci.data[Ci.strides[0] + 13] = C[29];
Ci.data[Ci.strides[0] + 14] = C[30];
Ci.data[Ci.strides[0] + 15] = C[31];
free(C);
}

// gemm_NEON_16x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 16] @DRAM
// )
void gemm_NEON_16x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[13] += C[13];
Ci.data[14] += C[14];
Ci.data[15] += C[15];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[Ci.strides[0] + 11] += C[27];
Ci.data[Ci.strides[0] + 12] += C[28];
Ci.data[Ci.strides[0] + 13] += C[29];
Ci.data[Ci.strides[0] + 14] += C[30];
Ci.data[Ci.strides[0] + 15] += C[31];
free(C);
}

// gemm_NEON_16x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 16] @DRAM
// )
void gemm_NEON_16x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[9] = C[9];
Ci.data[10] = C[10];
Ci.data[11] = C[11];
Ci.data[12] = C[12];
Ci.data[13] = C[13];
Ci.data[14] = C[14];
Ci.data[15] = C[15];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[Ci.strides[0] + 9] = C[25];
Ci.data[Ci.strides[0] + 10] = C[26];
Ci.data[Ci.strides[0] + 11] = C[27];
Ci.data[Ci.strides[0] + 12] = C[28];
Ci.data[Ci.strides[0] + 13] = C[29];
Ci.data[Ci.strides[0] + 14] = C[30];
Ci.data[Ci.strides[0] + 15] = C[31];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[2 * Ci.strides[0] + 9] = C[41];
Ci.data[2 * Ci.strides[0] + 10] = C[42];
Ci.data[2 * Ci.strides[0] + 11] = C[43];
Ci.data[2 * Ci.strides[0] + 12] = C[44];
Ci.data[2 * Ci.strides[0] + 13] = C[45];
Ci.data[2 * Ci.strides[0] + 14] = C[46];
Ci.data[2 * Ci.strides[0] + 15] = C[47];
free(C);
}

// gemm_NEON_16x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 16] @DRAM
// )
void gemm_NEON_16x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[9] += C[9];
Ci.data[10] += C[10];
Ci.data[11] += C[11];
Ci.data[12] += C[12];
Ci.data[13] += C[13];
Ci.data[14] += C[14];
Ci.data[15] += C[15];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[Ci.strides[0] + 9] += C[25];
Ci.data[Ci.strides[0] + 10] += C[26];
Ci.data[Ci.strides[0] + 11] += C[27];
Ci.data[Ci.strides[0] + 12] += C[28];
Ci.data[Ci.strides[0] + 13] += C[29];
Ci.data[Ci.strides[0] + 14] += C[30];
Ci.data[Ci.strides[0] + 15] += C[31];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[2 * Ci.strides[0] + 9] += C[41];
Ci.data[2 * Ci.strides[0] + 10] += C[42];
Ci.data[2 * Ci.strides[0] + 11] += C[43];
Ci.data[2 * Ci.strides[0] + 12] += C[44];
Ci.data[2 * Ci.strides[0] + 13] += C[45];
Ci.data[2 * Ci.strides[0] + 14] += C[46];
Ci.data[2 * Ci.strides[0] + 15] += C[47];
free(C);
}

// gemm_NEON_16x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     beta : i32[1] @DRAM,
//     C : [i32][4, 16] @DRAM
// )
void gemm_NEON_16x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* beta, struct exo_win_2i32 C ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C.data[0], C_reg_0_0);
vst1q_s32(&C.data[4], C_reg_0_1);
vst1q_s32(&C.data[8], C_reg_0_2);
vst1q_s32(&C.data[12], C_reg_0_3);
vst1q_s32(&C.data[C.strides[0]], C_reg_1_0);
vst1q_s32(&C.data[C.strides[0] + 4], C_reg_1_1);
vst1q_s32(&C.data[C.strides[0] + 8], C_reg_1_2);
vst1q_s32(&C.data[C.strides[0] + 12], C_reg_1_3);
vst1q_s32(&C.data[(2) * (C.strides[0])], C_reg_2_0);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 4], C_reg_2_1);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_2);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 12], C_reg_2_3);
vst1q_s32(&C.data[(3) * (C.strides[0])], C_reg_3_0);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 4], C_reg_3_1);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_2);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 12], C_reg_3_3);
}

// gemm_NEON_16x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     beta : i32[1] @DRAM,
//     C : [i32][4, 16] @DRAM
// )
void gemm_NEON_16x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* beta, struct exo_win_2i32 C ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vld1q_s32(&C.data[0]);
C_reg_0_1 = vld1q_s32(&C.data[4]);
C_reg_0_2 = vld1q_s32(&C.data[8]);
C_reg_0_3 = vld1q_s32(&C.data[12]);
C_reg_1_0 = vld1q_s32(&C.data[C.strides[0]]);
C_reg_1_1 = vld1q_s32(&C.data[C.strides[0] + 4]);
C_reg_1_2 = vld1q_s32(&C.data[C.strides[0] + 8]);
C_reg_1_3 = vld1q_s32(&C.data[C.strides[0] + 12]);
C_reg_2_0 = vld1q_s32(&C.data[(2) * (C.strides[0])]);
C_reg_2_1 = vld1q_s32(&C.data[(2) * (C.strides[0]) + 4]);
C_reg_2_2 = vld1q_s32(&C.data[(2) * (C.strides[0]) + 8]);
C_reg_2_3 = vld1q_s32(&C.data[(2) * (C.strides[0]) + 12]);
C_reg_3_0 = vld1q_s32(&C.data[(3) * (C.strides[0])]);
C_reg_3_1 = vld1q_s32(&C.data[(3) * (C.strides[0]) + 4]);
C_reg_3_2 = vld1q_s32(&C.data[(3) * (C.strides[0]) + 8]);
C_reg_3_3 = vld1q_s32(&C.data[(3) * (C.strides[0]) + 12]);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C.data[0], C_reg_0_0);
vst1q_s32(&C.data[4], C_reg_0_1);
vst1q_s32(&C.data[8], C_reg_0_2);
vst1q_s32(&C.data[12], C_reg_0_3);
vst1q_s32(&C.data[C.strides[0]], C_reg_1_0);
vst1q_s32(&C.data[C.strides[0] + 4], C_reg_1_1);
vst1q_s32(&C.data[C.strides[0] + 8], C_reg_1_2);
vst1q_s32(&C.data[C.strides[0] + 12], C_reg_1_3);
vst1q_s32(&C.data[(2) * (C.strides[0])], C_reg_2_0);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 4], C_reg_2_1);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_2);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 12], C_reg_2_3);
vst1q_s32(&C.data[(3) * (C.strides[0])], C_reg_3_0);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 4], C_reg_3_1);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_2);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 12], C_reg_3_3);
}

// gemm_NEON_1x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 1] @DRAM
// )
void gemm_NEON_1x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
free(C);
}

// gemm_NEON_1x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 1] @DRAM
// )
void gemm_NEON_1x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
free(C);
}

// gemm_NEON_1x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 1] @DRAM
// )
void gemm_NEON_1x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[Ci.strides[0]] = C[16];
free(C);
}

// gemm_NEON_1x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 1] @DRAM
// )
void gemm_NEON_1x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[Ci.strides[0]] += C[16];
free(C);
}

// gemm_NEON_1x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 1] @DRAM
// )
void gemm_NEON_1x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[2 * Ci.strides[0]] = C[32];
free(C);
}

// gemm_NEON_1x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 1] @DRAM
// )
void gemm_NEON_1x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[2 * Ci.strides[0]] += C[32];
free(C);
}

// gemm_NEON_1x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 1] @DRAM
// )
void gemm_NEON_1x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[3 * Ci.strides[0]] = C[48];
free(C);
}

// gemm_NEON_1x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 1] @DRAM
// )
void gemm_NEON_1x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[3 * Ci.strides[0]] += C[48];
free(C);
}

// gemm_NEON_2x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 2] @DRAM
// )
void gemm_NEON_2x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
free(C);
}

// gemm_NEON_2x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 2] @DRAM
// )
void gemm_NEON_2x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
free(C);
}

// gemm_NEON_2x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 2] @DRAM
// )
void gemm_NEON_2x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
free(C);
}

// gemm_NEON_2x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 2] @DRAM
// )
void gemm_NEON_2x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
free(C);
}

// gemm_NEON_2x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 2] @DRAM
// )
void gemm_NEON_2x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
free(C);
}

// gemm_NEON_2x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 2] @DRAM
// )
void gemm_NEON_2x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
free(C);
}

// gemm_NEON_2x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 2] @DRAM
// )
void gemm_NEON_2x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[3 * Ci.strides[0]] = C[48];
Ci.data[3 * Ci.strides[0] + 1] = C[49];
free(C);
}

// gemm_NEON_2x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 2] @DRAM
// )
void gemm_NEON_2x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[3 * Ci.strides[0]] += C[48];
Ci.data[3 * Ci.strides[0] + 1] += C[49];
free(C);
}

// gemm_NEON_3x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 3] @DRAM
// )
void gemm_NEON_3x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
free(C);
}

// gemm_NEON_3x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 3] @DRAM
// )
void gemm_NEON_3x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
free(C);
}

// gemm_NEON_3x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 3] @DRAM
// )
void gemm_NEON_3x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
free(C);
}

// gemm_NEON_3x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 3] @DRAM
// )
void gemm_NEON_3x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
free(C);
}

// gemm_NEON_3x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 3] @DRAM
// )
void gemm_NEON_3x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
free(C);
}

// gemm_NEON_3x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 3] @DRAM
// )
void gemm_NEON_3x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
free(C);
}

// gemm_NEON_3x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 3] @DRAM
// )
void gemm_NEON_3x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[3 * Ci.strides[0]] = C[48];
Ci.data[3 * Ci.strides[0] + 1] = C[49];
Ci.data[3 * Ci.strides[0] + 2] = C[50];
free(C);
}

// gemm_NEON_3x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 3] @DRAM
// )
void gemm_NEON_3x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[3 * Ci.strides[0]] += C[48];
Ci.data[3 * Ci.strides[0] + 1] += C[49];
Ci.data[3 * Ci.strides[0] + 2] += C[50];
free(C);
}

// gemm_NEON_4x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 4] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 4] @DRAM
// )
void gemm_NEON_4x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 4
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 4 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_1_0;
int32x4_t C_reg_2_0;
int32x4_t C_reg_3_0;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_1_0);
vst1q_s32(&C[(2) * 4], C_reg_2_0);
vst1q_s32(&C[(3) * 4], C_reg_3_0);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
free(C);
}

// gemm_NEON_4x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 4] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 4] @DRAM
// )
void gemm_NEON_4x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 4
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 4 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_1_0;
int32x4_t C_reg_2_0;
int32x4_t C_reg_3_0;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_1_0);
vst1q_s32(&C[(2) * 4], C_reg_2_0);
vst1q_s32(&C[(3) * 4], C_reg_3_0);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
free(C);
}

// gemm_NEON_4x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 4] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 4] @DRAM
// )
void gemm_NEON_4x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 4
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 4 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_1_0;
int32x4_t C_reg_2_0;
int32x4_t C_reg_3_0;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_1_0);
vst1q_s32(&C[(2) * 4], C_reg_2_0);
vst1q_s32(&C[(3) * 4], C_reg_3_0);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[Ci.strides[0]] = C[4];
Ci.data[Ci.strides[0] + 1] = C[5];
Ci.data[Ci.strides[0] + 2] = C[6];
Ci.data[Ci.strides[0] + 3] = C[7];
free(C);
}

// gemm_NEON_4x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 4] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 4] @DRAM
// )
void gemm_NEON_4x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 4
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 4 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_1_0;
int32x4_t C_reg_2_0;
int32x4_t C_reg_3_0;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_1_0);
vst1q_s32(&C[(2) * 4], C_reg_2_0);
vst1q_s32(&C[(3) * 4], C_reg_3_0);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[Ci.strides[0]] += C[4];
Ci.data[Ci.strides[0] + 1] += C[5];
Ci.data[Ci.strides[0] + 2] += C[6];
Ci.data[Ci.strides[0] + 3] += C[7];
free(C);
}

// gemm_NEON_4x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 4] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 4] @DRAM
// )
void gemm_NEON_4x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 4
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 4 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_1_0;
int32x4_t C_reg_2_0;
int32x4_t C_reg_3_0;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_1_0);
vst1q_s32(&C[(2) * 4], C_reg_2_0);
vst1q_s32(&C[(3) * 4], C_reg_3_0);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[Ci.strides[0]] = C[4];
Ci.data[Ci.strides[0] + 1] = C[5];
Ci.data[Ci.strides[0] + 2] = C[6];
Ci.data[Ci.strides[0] + 3] = C[7];
Ci.data[2 * Ci.strides[0]] = C[8];
Ci.data[2 * Ci.strides[0] + 1] = C[9];
Ci.data[2 * Ci.strides[0] + 2] = C[10];
Ci.data[2 * Ci.strides[0] + 3] = C[11];
free(C);
}

// gemm_NEON_4x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 4] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 4] @DRAM
// )
void gemm_NEON_4x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 4
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 4 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_1_0;
int32x4_t C_reg_2_0;
int32x4_t C_reg_3_0;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 4]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_1_0);
vst1q_s32(&C[(2) * 4], C_reg_2_0);
vst1q_s32(&C[(3) * 4], C_reg_3_0);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[Ci.strides[0]] += C[4];
Ci.data[Ci.strides[0] + 1] += C[5];
Ci.data[Ci.strides[0] + 2] += C[6];
Ci.data[Ci.strides[0] + 3] += C[7];
Ci.data[2 * Ci.strides[0]] += C[8];
Ci.data[2 * Ci.strides[0] + 1] += C[9];
Ci.data[2 * Ci.strides[0] + 2] += C[10];
Ci.data[2 * Ci.strides[0] + 3] += C[11];
free(C);
}

// gemm_NEON_4x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 4] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     beta : i32[1] @DRAM,
//     C : [i32][4, 4] @DRAM
// )
void gemm_NEON_4x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* beta, struct exo_win_2i32 C ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
int32x4_t C_reg_0_0;
int32x4_t C_reg_1_0;
int32x4_t C_reg_2_0;
int32x4_t C_reg_3_0;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
vst1q_s32(&C.data[0], C_reg_0_0);
vst1q_s32(&C.data[C.strides[0]], C_reg_1_0);
vst1q_s32(&C.data[(2) * (C.strides[0])], C_reg_2_0);
vst1q_s32(&C.data[(3) * (C.strides[0])], C_reg_3_0);
}

// gemm_NEON_4x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 4] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     beta : i32[1] @DRAM,
//     C : [i32][4, 4] @DRAM
// )
void gemm_NEON_4x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* beta, struct exo_win_2i32 C ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
int32x4_t C_reg_0_0;
int32x4_t C_reg_1_0;
int32x4_t C_reg_2_0;
int32x4_t C_reg_3_0;
C_reg_0_0 = vld1q_s32(&C.data[0]);
C_reg_1_0 = vld1q_s32(&C.data[C.strides[0]]);
C_reg_2_0 = vld1q_s32(&C.data[(2) * (C.strides[0])]);
C_reg_3_0 = vld1q_s32(&C.data[(3) * (C.strides[0])]);
int16x4_t A_reg_0;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
}
vst1q_s32(&C.data[0], C_reg_0_0);
vst1q_s32(&C.data[C.strides[0]], C_reg_1_0);
vst1q_s32(&C.data[(2) * (C.strides[0])], C_reg_2_0);
vst1q_s32(&C.data[(3) * (C.strides[0])], C_reg_3_0);
}

// gemm_NEON_5x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 5] @DRAM
// )
void gemm_NEON_5x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
free(C);
}

// gemm_NEON_5x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 5] @DRAM
// )
void gemm_NEON_5x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
free(C);
}

// gemm_NEON_5x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 5] @DRAM
// )
void gemm_NEON_5x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
free(C);
}

// gemm_NEON_5x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 5] @DRAM
// )
void gemm_NEON_5x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
free(C);
}

// gemm_NEON_5x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 5] @DRAM
// )
void gemm_NEON_5x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
free(C);
}

// gemm_NEON_5x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 5] @DRAM
// )
void gemm_NEON_5x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
free(C);
}

// gemm_NEON_5x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 5] @DRAM
// )
void gemm_NEON_5x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[3 * Ci.strides[0]] = C[48];
Ci.data[3 * Ci.strides[0] + 1] = C[49];
Ci.data[3 * Ci.strides[0] + 2] = C[50];
Ci.data[3 * Ci.strides[0] + 3] = C[51];
Ci.data[3 * Ci.strides[0] + 4] = C[52];
free(C);
}

// gemm_NEON_5x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 5] @DRAM
// )
void gemm_NEON_5x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[3 * Ci.strides[0]] += C[48];
Ci.data[3 * Ci.strides[0] + 1] += C[49];
Ci.data[3 * Ci.strides[0] + 2] += C[50];
Ci.data[3 * Ci.strides[0] + 3] += C[51];
Ci.data[3 * Ci.strides[0] + 4] += C[52];
free(C);
}

// gemm_NEON_6x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 6] @DRAM
// )
void gemm_NEON_6x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
free(C);
}

// gemm_NEON_6x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 6] @DRAM
// )
void gemm_NEON_6x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
free(C);
}

// gemm_NEON_6x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 6] @DRAM
// )
void gemm_NEON_6x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
free(C);
}

// gemm_NEON_6x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 6] @DRAM
// )
void gemm_NEON_6x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
free(C);
}

// gemm_NEON_6x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 6] @DRAM
// )
void gemm_NEON_6x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
free(C);
}

// gemm_NEON_6x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 6] @DRAM
// )
void gemm_NEON_6x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
free(C);
}

// gemm_NEON_6x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 6] @DRAM
// )
void gemm_NEON_6x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[3 * Ci.strides[0]] = C[48];
Ci.data[3 * Ci.strides[0] + 1] = C[49];
Ci.data[3 * Ci.strides[0] + 2] = C[50];
Ci.data[3 * Ci.strides[0] + 3] = C[51];
Ci.data[3 * Ci.strides[0] + 4] = C[52];
Ci.data[3 * Ci.strides[0] + 5] = C[53];
free(C);
}

// gemm_NEON_6x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 6] @DRAM
// )
void gemm_NEON_6x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[3 * Ci.strides[0]] += C[48];
Ci.data[3 * Ci.strides[0] + 1] += C[49];
Ci.data[3 * Ci.strides[0] + 2] += C[50];
Ci.data[3 * Ci.strides[0] + 3] += C[51];
Ci.data[3 * Ci.strides[0] + 4] += C[52];
Ci.data[3 * Ci.strides[0] + 5] += C[53];
free(C);
}

// gemm_NEON_7x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 7] @DRAM
// )
void gemm_NEON_7x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
free(C);
}

// gemm_NEON_7x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 7] @DRAM
// )
void gemm_NEON_7x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
free(C);
}

// gemm_NEON_7x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 7] @DRAM
// )
void gemm_NEON_7x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
free(C);
}

// gemm_NEON_7x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 7] @DRAM
// )
void gemm_NEON_7x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
free(C);
}

// gemm_NEON_7x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 7] @DRAM
// )
void gemm_NEON_7x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
free(C);
}

// gemm_NEON_7x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 7] @DRAM
// )
void gemm_NEON_7x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
free(C);
}

// gemm_NEON_7x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 7] @DRAM
// )
void gemm_NEON_7x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[3 * Ci.strides[0]] = C[48];
Ci.data[3 * Ci.strides[0] + 1] = C[49];
Ci.data[3 * Ci.strides[0] + 2] = C[50];
Ci.data[3 * Ci.strides[0] + 3] = C[51];
Ci.data[3 * Ci.strides[0] + 4] = C[52];
Ci.data[3 * Ci.strides[0] + 5] = C[53];
Ci.data[3 * Ci.strides[0] + 6] = C[54];
free(C);
}

// gemm_NEON_7x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 7] @DRAM
// )
void gemm_NEON_7x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[3 * Ci.strides[0]] += C[48];
Ci.data[3 * Ci.strides[0] + 1] += C[49];
Ci.data[3 * Ci.strides[0] + 2] += C[50];
Ci.data[3 * Ci.strides[0] + 3] += C[51];
Ci.data[3 * Ci.strides[0] + 4] += C[52];
Ci.data[3 * Ci.strides[0] + 5] += C[53];
Ci.data[3 * Ci.strides[0] + 6] += C[54];
free(C);
}

// gemm_NEON_8x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 8] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 8] @DRAM
// )
void gemm_NEON_8x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 8
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 8 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_1_0);
vst1q_s32(&C[8 + 4], C_reg_1_1);
vst1q_s32(&C[(2) * 8], C_reg_2_0);
vst1q_s32(&C[(2) * 8 + 4], C_reg_2_1);
vst1q_s32(&C[(3) * 8], C_reg_3_0);
vst1q_s32(&C[(3) * 8 + 4], C_reg_3_1);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
free(C);
}

// gemm_NEON_8x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 8] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 8] @DRAM
// )
void gemm_NEON_8x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 8
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 8 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_1_0);
vst1q_s32(&C[8 + 4], C_reg_1_1);
vst1q_s32(&C[(2) * 8], C_reg_2_0);
vst1q_s32(&C[(2) * 8 + 4], C_reg_2_1);
vst1q_s32(&C[(3) * 8], C_reg_3_0);
vst1q_s32(&C[(3) * 8 + 4], C_reg_3_1);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
free(C);
}

// gemm_NEON_8x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 8] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 8] @DRAM
// )
void gemm_NEON_8x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 8
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 8 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_1_0);
vst1q_s32(&C[8 + 4], C_reg_1_1);
vst1q_s32(&C[(2) * 8], C_reg_2_0);
vst1q_s32(&C[(2) * 8 + 4], C_reg_2_1);
vst1q_s32(&C[(3) * 8], C_reg_3_0);
vst1q_s32(&C[(3) * 8 + 4], C_reg_3_1);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[Ci.strides[0]] = C[8];
Ci.data[Ci.strides[0] + 1] = C[9];
Ci.data[Ci.strides[0] + 2] = C[10];
Ci.data[Ci.strides[0] + 3] = C[11];
Ci.data[Ci.strides[0] + 4] = C[12];
Ci.data[Ci.strides[0] + 5] = C[13];
Ci.data[Ci.strides[0] + 6] = C[14];
Ci.data[Ci.strides[0] + 7] = C[15];
free(C);
}

// gemm_NEON_8x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 8] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 8] @DRAM
// )
void gemm_NEON_8x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 8
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 8 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_1_0);
vst1q_s32(&C[8 + 4], C_reg_1_1);
vst1q_s32(&C[(2) * 8], C_reg_2_0);
vst1q_s32(&C[(2) * 8 + 4], C_reg_2_1);
vst1q_s32(&C[(3) * 8], C_reg_3_0);
vst1q_s32(&C[(3) * 8 + 4], C_reg_3_1);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[Ci.strides[0]] += C[8];
Ci.data[Ci.strides[0] + 1] += C[9];
Ci.data[Ci.strides[0] + 2] += C[10];
Ci.data[Ci.strides[0] + 3] += C[11];
Ci.data[Ci.strides[0] + 4] += C[12];
Ci.data[Ci.strides[0] + 5] += C[13];
Ci.data[Ci.strides[0] + 6] += C[14];
Ci.data[Ci.strides[0] + 7] += C[15];
free(C);
}

// gemm_NEON_8x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 8] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 8] @DRAM
// )
void gemm_NEON_8x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 8
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 8 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_1_0);
vst1q_s32(&C[8 + 4], C_reg_1_1);
vst1q_s32(&C[(2) * 8], C_reg_2_0);
vst1q_s32(&C[(2) * 8 + 4], C_reg_2_1);
vst1q_s32(&C[(3) * 8], C_reg_3_0);
vst1q_s32(&C[(3) * 8 + 4], C_reg_3_1);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[Ci.strides[0]] = C[8];
Ci.data[Ci.strides[0] + 1] = C[9];
Ci.data[Ci.strides[0] + 2] = C[10];
Ci.data[Ci.strides[0] + 3] = C[11];
Ci.data[Ci.strides[0] + 4] = C[12];
Ci.data[Ci.strides[0] + 5] = C[13];
Ci.data[Ci.strides[0] + 6] = C[14];
Ci.data[Ci.strides[0] + 7] = C[15];
Ci.data[2 * Ci.strides[0]] = C[16];
Ci.data[2 * Ci.strides[0] + 1] = C[17];
Ci.data[2 * Ci.strides[0] + 2] = C[18];
Ci.data[2 * Ci.strides[0] + 3] = C[19];
Ci.data[2 * Ci.strides[0] + 4] = C[20];
Ci.data[2 * Ci.strides[0] + 5] = C[21];
Ci.data[2 * Ci.strides[0] + 6] = C[22];
Ci.data[2 * Ci.strides[0] + 7] = C[23];
free(C);
}

// gemm_NEON_8x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 8] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 8] @DRAM
// )
void gemm_NEON_8x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 8
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 8 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_1_0);
vst1q_s32(&C[8 + 4], C_reg_1_1);
vst1q_s32(&C[(2) * 8], C_reg_2_0);
vst1q_s32(&C[(2) * 8 + 4], C_reg_2_1);
vst1q_s32(&C[(3) * 8], C_reg_3_0);
vst1q_s32(&C[(3) * 8 + 4], C_reg_3_1);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[Ci.strides[0]] += C[8];
Ci.data[Ci.strides[0] + 1] += C[9];
Ci.data[Ci.strides[0] + 2] += C[10];
Ci.data[Ci.strides[0] + 3] += C[11];
Ci.data[Ci.strides[0] + 4] += C[12];
Ci.data[Ci.strides[0] + 5] += C[13];
Ci.data[Ci.strides[0] + 6] += C[14];
Ci.data[Ci.strides[0] + 7] += C[15];
Ci.data[2 * Ci.strides[0]] += C[16];
Ci.data[2 * Ci.strides[0] + 1] += C[17];
Ci.data[2 * Ci.strides[0] + 2] += C[18];
Ci.data[2 * Ci.strides[0] + 3] += C[19];
Ci.data[2 * Ci.strides[0] + 4] += C[20];
Ci.data[2 * Ci.strides[0] + 5] += C[21];
Ci.data[2 * Ci.strides[0] + 6] += C[22];
Ci.data[2 * Ci.strides[0] + 7] += C[23];
free(C);
}

// gemm_NEON_8x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 8] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     beta : i32[1] @DRAM,
//     C : [i32][4, 8] @DRAM
// )
void gemm_NEON_8x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* beta, struct exo_win_2i32 C ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
vst1q_s32(&C.data[0], C_reg_0_0);
vst1q_s32(&C.data[4], C_reg_0_1);
vst1q_s32(&C.data[C.strides[0]], C_reg_1_0);
vst1q_s32(&C.data[C.strides[0] + 4], C_reg_1_1);
vst1q_s32(&C.data[(2) * (C.strides[0])], C_reg_2_0);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 4], C_reg_2_1);
vst1q_s32(&C.data[(3) * (C.strides[0])], C_reg_3_0);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 4], C_reg_3_1);
}

// gemm_NEON_8x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 8] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     beta : i32[1] @DRAM,
//     C : [i32][4, 8] @DRAM
// )
void gemm_NEON_8x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* beta, struct exo_win_2i32 C ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
C_reg_0_0 = vld1q_s32(&C.data[0]);
C_reg_0_1 = vld1q_s32(&C.data[4]);
C_reg_1_0 = vld1q_s32(&C.data[C.strides[0]]);
C_reg_1_1 = vld1q_s32(&C.data[C.strides[0] + 4]);
C_reg_2_0 = vld1q_s32(&C.data[(2) * (C.strides[0])]);
C_reg_2_1 = vld1q_s32(&C.data[(2) * (C.strides[0]) + 4]);
C_reg_3_0 = vld1q_s32(&C.data[(3) * (C.strides[0])]);
C_reg_3_1 = vld1q_s32(&C.data[(3) * (C.strides[0]) + 4]);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int8x8_t A_temp_0;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
}
vst1q_s32(&C.data[0], C_reg_0_0);
vst1q_s32(&C.data[4], C_reg_0_1);
vst1q_s32(&C.data[C.strides[0]], C_reg_1_0);
vst1q_s32(&C.data[C.strides[0] + 4], C_reg_1_1);
vst1q_s32(&C.data[(2) * (C.strides[0])], C_reg_2_0);
vst1q_s32(&C.data[(2) * (C.strides[0]) + 4], C_reg_2_1);
vst1q_s32(&C.data[(3) * (C.strides[0])], C_reg_3_0);
vst1q_s32(&C.data[(3) * (C.strides[0]) + 4], C_reg_3_1);
}

// gemm_NEON_9x1_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 9] @DRAM
// )
void gemm_NEON_9x1_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
free(C);
}

// gemm_NEON_9x1_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][1, 9] @DRAM
// )
void gemm_NEON_9x1_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
free(C);
}

// gemm_NEON_9x2_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 9] @DRAM
// )
void gemm_NEON_9x2_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
free(C);
}

// gemm_NEON_9x2_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][2, 9] @DRAM
// )
void gemm_NEON_9x2_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
free(C);
}

// gemm_NEON_9x3_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 9] @DRAM
// )
void gemm_NEON_9x3_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
free(C);
}

// gemm_NEON_9x3_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][3, 9] @DRAM
// )
void gemm_NEON_9x3_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
free(C);
}

// gemm_NEON_9x4_b0_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 9] @DRAM
// )
void gemm_NEON_9x4_b0_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] = C[0];
Ci.data[1] = C[1];
Ci.data[2] = C[2];
Ci.data[3] = C[3];
Ci.data[4] = C[4];
Ci.data[5] = C[5];
Ci.data[6] = C[6];
Ci.data[7] = C[7];
Ci.data[8] = C[8];
Ci.data[Ci.strides[0]] = C[16];
Ci.data[Ci.strides[0] + 1] = C[17];
Ci.data[Ci.strides[0] + 2] = C[18];
Ci.data[Ci.strides[0] + 3] = C[19];
Ci.data[Ci.strides[0] + 4] = C[20];
Ci.data[Ci.strides[0] + 5] = C[21];
Ci.data[Ci.strides[0] + 6] = C[22];
Ci.data[Ci.strides[0] + 7] = C[23];
Ci.data[Ci.strides[0] + 8] = C[24];
Ci.data[2 * Ci.strides[0]] = C[32];
Ci.data[2 * Ci.strides[0] + 1] = C[33];
Ci.data[2 * Ci.strides[0] + 2] = C[34];
Ci.data[2 * Ci.strides[0] + 3] = C[35];
Ci.data[2 * Ci.strides[0] + 4] = C[36];
Ci.data[2 * Ci.strides[0] + 5] = C[37];
Ci.data[2 * Ci.strides[0] + 6] = C[38];
Ci.data[2 * Ci.strides[0] + 7] = C[39];
Ci.data[2 * Ci.strides[0] + 8] = C[40];
Ci.data[3 * Ci.strides[0]] = C[48];
Ci.data[3 * Ci.strides[0] + 1] = C[49];
Ci.data[3 * Ci.strides[0] + 2] = C[50];
Ci.data[3 * Ci.strides[0] + 3] = C[51];
Ci.data[3 * Ci.strides[0] + 4] = C[52];
Ci.data[3 * Ci.strides[0] + 5] = C[53];
Ci.data[3 * Ci.strides[0] + 6] = C[54];
Ci.data[3 * Ci.strides[0] + 7] = C[55];
Ci.data[3 * Ci.strides[0] + 8] = C[56];
free(C);
}

// gemm_NEON_9x4_b1_col_i32(
//     KC : size,
//     alpha : i32[1] @DRAM,
//     A : [i8][KC, 16] @DRAM,
//     B : [i8][KC, 4] @DRAM,
//     b : i32[1] @DRAM,
//     Ci : [i32][4, 9] @DRAM
// )
void gemm_NEON_9x4_b1_col_i32( void *ctxt, int_fast32_t KC, const int32_t* alpha, struct exo_win_2i8c A, struct exo_win_2i8c B, const int32_t* b, struct exo_win_2i32 Ci ) {
// assert stride(A, 0) == 16
// assert stride(A, 1) == 1
// assert stride(B, 0) == 4
// assert stride(B, 1) == 1
// assert stride(Ci, 1) == 1
int32_t *C = (int32_t*) malloc(4 * 16 * sizeof(*C));
int32x4_t C_reg_0_0;
int32x4_t C_reg_0_1;
int32x4_t C_reg_0_2;
int32x4_t C_reg_0_3;
int32x4_t C_reg_1_0;
int32x4_t C_reg_1_1;
int32x4_t C_reg_1_2;
int32x4_t C_reg_1_3;
int32x4_t C_reg_2_0;
int32x4_t C_reg_2_1;
int32x4_t C_reg_2_2;
int32x4_t C_reg_2_3;
int32x4_t C_reg_3_0;
int32x4_t C_reg_3_1;
int32x4_t C_reg_3_2;
int32x4_t C_reg_3_3;
C_reg_0_0 = vmovq_n_s32(0);
C_reg_0_1 = vmovq_n_s32(0);
C_reg_0_2 = vmovq_n_s32(0);
C_reg_0_3 = vmovq_n_s32(0);
C_reg_1_0 = vmovq_n_s32(0);
C_reg_1_1 = vmovq_n_s32(0);
C_reg_1_2 = vmovq_n_s32(0);
C_reg_1_3 = vmovq_n_s32(0);
C_reg_2_0 = vmovq_n_s32(0);
C_reg_2_1 = vmovq_n_s32(0);
C_reg_2_2 = vmovq_n_s32(0);
C_reg_2_3 = vmovq_n_s32(0);
C_reg_3_0 = vmovq_n_s32(0);
C_reg_3_1 = vmovq_n_s32(0);
C_reg_3_2 = vmovq_n_s32(0);
C_reg_3_3 = vmovq_n_s32(0);
int16x4_t A_reg_0;
int16x4_t A_reg_1;
int16x4_t A_reg_2;
int16x4_t A_reg_3;
int8x8_t A_temp_0;
int8x8_t A_temp_1;
int16x4_t B_reg[1];
int8x8_t B_temp_0;
for (int_fast32_t kt = 0; kt < ((KC) / (4)); kt++) {
  A_temp_0 = vld1_s8(&A.data[(4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(1 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(1 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(1 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(2 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(2 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(2 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
  A_temp_0 = vld1_s8(&A.data[(3 + 4 * kt) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(3 + 4 * kt) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(3 + 4 * kt) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
for (int_fast32_t ktt = 0; ktt < KC % 4; ktt++) {
  A_temp_0 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16)]);
  A_temp_1 = vld1_s8(&A.data[(ktt + (KC / 4) * 4) * (16) + 8]);
  A_reg_0 = vget_low_s16(vmovl_s8(A_temp_0));
  A_reg_1 = vget_high_s16(vmovl_s8(A_temp_0));
  A_reg_2 = vget_low_s16(vmovl_s8(A_temp_1));
  A_reg_3 = vget_high_s16(vmovl_s8(A_temp_1));
  B_temp_0 = vld1_s8(&B.data[(ktt + (KC / 4) * 4) * 4]);
  B_reg[0] = vget_low_s16(vmovl_s8(B_temp_0));
  C_reg_0_0 = vmlal_lane_s16(C_reg_0_0, A_reg_0, B_reg[0], (0));
  C_reg_1_0 = vmlal_lane_s16(C_reg_1_0, A_reg_0, B_reg[0], (1));
  C_reg_2_0 = vmlal_lane_s16(C_reg_2_0, A_reg_0, B_reg[0], (2));
  C_reg_3_0 = vmlal_lane_s16(C_reg_3_0, A_reg_0, B_reg[0], (3));
  C_reg_0_1 = vmlal_lane_s16(C_reg_0_1, A_reg_1, B_reg[0], (0));
  C_reg_1_1 = vmlal_lane_s16(C_reg_1_1, A_reg_1, B_reg[0], (1));
  C_reg_2_1 = vmlal_lane_s16(C_reg_2_1, A_reg_1, B_reg[0], (2));
  C_reg_3_1 = vmlal_lane_s16(C_reg_3_1, A_reg_1, B_reg[0], (3));
  C_reg_0_2 = vmlal_lane_s16(C_reg_0_2, A_reg_2, B_reg[0], (0));
  C_reg_1_2 = vmlal_lane_s16(C_reg_1_2, A_reg_2, B_reg[0], (1));
  C_reg_2_2 = vmlal_lane_s16(C_reg_2_2, A_reg_2, B_reg[0], (2));
  C_reg_3_2 = vmlal_lane_s16(C_reg_3_2, A_reg_2, B_reg[0], (3));
  C_reg_0_3 = vmlal_lane_s16(C_reg_0_3, A_reg_3, B_reg[0], (0));
  C_reg_1_3 = vmlal_lane_s16(C_reg_1_3, A_reg_3, B_reg[0], (1));
  C_reg_2_3 = vmlal_lane_s16(C_reg_2_3, A_reg_3, B_reg[0], (2));
  C_reg_3_3 = vmlal_lane_s16(C_reg_3_3, A_reg_3, B_reg[0], (3));
}
vst1q_s32(&C[0], C_reg_0_0);
vst1q_s32(&C[4], C_reg_0_1);
vst1q_s32(&C[8], C_reg_0_2);
vst1q_s32(&C[12], C_reg_0_3);
vst1q_s32(&C[16], C_reg_1_0);
vst1q_s32(&C[16 + 4], C_reg_1_1);
vst1q_s32(&C[16 + 8], C_reg_1_2);
vst1q_s32(&C[16 + 12], C_reg_1_3);
vst1q_s32(&C[(2) * (16)], C_reg_2_0);
vst1q_s32(&C[(2) * (16) + 4], C_reg_2_1);
vst1q_s32(&C[(2) * (16) + 8], C_reg_2_2);
vst1q_s32(&C[(2) * (16) + 12], C_reg_2_3);
vst1q_s32(&C[(3) * (16)], C_reg_3_0);
vst1q_s32(&C[(3) * (16) + 4], C_reg_3_1);
vst1q_s32(&C[(3) * (16) + 8], C_reg_3_2);
vst1q_s32(&C[(3) * (16) + 12], C_reg_3_3);
Ci.data[0] += C[0];
Ci.data[1] += C[1];
Ci.data[2] += C[2];
Ci.data[3] += C[3];
Ci.data[4] += C[4];
Ci.data[5] += C[5];
Ci.data[6] += C[6];
Ci.data[7] += C[7];
Ci.data[8] += C[8];
Ci.data[Ci.strides[0]] += C[16];
Ci.data[Ci.strides[0] + 1] += C[17];
Ci.data[Ci.strides[0] + 2] += C[18];
Ci.data[Ci.strides[0] + 3] += C[19];
Ci.data[Ci.strides[0] + 4] += C[20];
Ci.data[Ci.strides[0] + 5] += C[21];
Ci.data[Ci.strides[0] + 6] += C[22];
Ci.data[Ci.strides[0] + 7] += C[23];
Ci.data[Ci.strides[0] + 8] += C[24];
Ci.data[2 * Ci.strides[0]] += C[32];
Ci.data[2 * Ci.strides[0] + 1] += C[33];
Ci.data[2 * Ci.strides[0] + 2] += C[34];
Ci.data[2 * Ci.strides[0] + 3] += C[35];
Ci.data[2 * Ci.strides[0] + 4] += C[36];
Ci.data[2 * Ci.strides[0] + 5] += C[37];
Ci.data[2 * Ci.strides[0] + 6] += C[38];
Ci.data[2 * Ci.strides[0] + 7] += C[39];
Ci.data[2 * Ci.strides[0] + 8] += C[40];
Ci.data[3 * Ci.strides[0]] += C[48];
Ci.data[3 * Ci.strides[0] + 1] += C[49];
Ci.data[3 * Ci.strides[0] + 2] += C[50];
Ci.data[3 * Ci.strides[0] + 3] += C[51];
Ci.data[3 * Ci.strides[0] + 4] += C[52];
Ci.data[3 * Ci.strides[0] + 5] += C[53];
Ci.data[3 * Ci.strides[0] + 6] += C[54];
Ci.data[3 * Ci.strides[0] + 7] += C[55];
Ci.data[3 * Ci.strides[0] + 8] += C[56];
free(C);
}


/* relying on the following instruction..."
neon_get_high_8xi16(dst,src)
{dst_data} = vget_high_s16(vmovl_s8({src_data}));
*/

/* relying on the following instruction..."
neon_get_low_8xi16(dst,src)
{dst_data} = vget_low_s16(vmovl_s8({src_data}));
*/

/* relying on the following instruction..."
neon_vld_4xi32(dst,src)
{dst_data} = vld1q_s32(&{src_data});
*/

/* relying on the following instruction..."
neon_vld_8xi8(dst,src,e)
{dst_data} = vld1_s8(&{src_data});
*/

/* relying on the following instruction..."
neon_vmlal_8xi16_8xi16(dst,lhs,rhs,jtt)
{dst_data} = vmlal_lane_s16({dst_data}, {lhs_data}, {rhs_data}, {jtt});
*/

/* relying on the following instruction..."
neon_vst_4xi32(dst,src)
vst1q_s32(&{dst_data}, {src_data});
*/

/* relying on the following instruction..."
neon_zero_4xi32(dst)
{dst_data} = vmovq_n_s32(0);
*/
